{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "import scipy.signal\n",
    "import pywt\n",
    "from scipy.signal import butter, lfilter\n",
    "import json\n",
    "from scipy.stats import kurtosis, skew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        piezo_data = data.get('piezo', [])\n",
    "        piezo_set = np.array(piezo_data)\n",
    "        mic_data = data.get('mic', [])\n",
    "        mic_set = np.array(mic_data)\n",
    "\n",
    "        denoised_piezo_data = wavelet_denoise(piezo_set, wavelet='db2', level=2) # 这里使用了Daubechies小波（'db1'），也就是Haar小波，分解层数为1。\n",
    "        denoised_mic_data = wavelet_denoise(mic_set, wavelet='db2', level=2)\n",
    "        return denoised_piezo_data, denoised_mic_data\n",
    "    # return piezo_set, mic_set\n",
    "\n",
    "def signal_visualization(piezo, mic, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(piezo, linestyle='-', label='piezo Data') #可视化一个拳头敲击周期的动作\n",
    "    plt.plot(mic, linestyle='-', label='mic Data')\n",
    "    plt.title(f'Sensor Data Over Time for {filename}')\n",
    "    plt.xlabel('Time (arbitrary units)')\n",
    "    plt.ylabel('Sensor Value')\n",
    "    # plt.ylim(0,6000)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def fft_visualization(signal, fft_result, filename):\n",
    "\n",
    "    fs = 6000\n",
    "    \n",
    "    frequencies = np.fft.fftfreq(len(fft_result), 1/fs)\n",
    "    magnitude = np.abs(fft_result)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(frequencies, magnitude)\n",
    "    plt.title(f'FFT of {filename}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, fs / 2)  # Limit x-axis to positive frequencies only up to Nyquist frequency\n",
    "    plt.show()\n",
    "\n",
    "    return frequencies, magnitude\n",
    "\n",
    "def fft_features(fft_results):\n",
    "    # Assume fft_results is an array of FFT coefficients\n",
    "    magnitude_spectrum = np.abs(fft_results)\n",
    "    print(magnitude_spectrum)\n",
    "    \n",
    "    # Spectral Centroid： Indicates the \"center of mass\" of the spectrum, giving an idea of the \"brightness\" of a sound.\n",
    "    spectral_centroid = np.sum(magnitude_spectrum * np.arange(len(magnitude_spectrum))) / np.sum(magnitude_spectrum)\n",
    "    \n",
    "    # Spectral Rolloff：The frequency below which a certain percentage of the total spectral energy (commonly 85% or 95%) is contained, which helps in differentiating between harmonic content and noise.\n",
    "    spectral_rolloff_threshold = 0.85 * np.sum(magnitude_spectrum)\n",
    "    cumulative_sum = np.cumsum(magnitude_spectrum)\n",
    "    spectral_rolloff = np.where(cumulative_sum >= spectral_rolloff_threshold)[0][0]\n",
    "    \n",
    "    # Spectral Flux：Measures the amount of local spectral change between successive frames, useful for detecting events.\n",
    "    spectral_flux = np.sum((np.diff(magnitude_spectrum) ** 2))\n",
    "    \n",
    "    # Total Spectral Energy：Sum of squares of the FFT coefficients can serve as a measure of the overall signal energy.\n",
    "    total_spectral_energy = np.sum(magnitude_spectrum ** 2)\n",
    "    \n",
    "    return {\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'spectral_rolloff': spectral_rolloff,\n",
    "        'spectral_flux': spectral_flux,\n",
    "        'total_spectral_energy': total_spectral_energy\n",
    "    }\n",
    "\n",
    "def wavelet_features(signal):\n",
    "    wavelet_transform = scipy.signal.cwt(signal, scipy.signal.ricker, widths=np.arange(1, 31))\n",
    "    wavelet_energy = np.sum(wavelet_transform**2, axis=0)\n",
    "    \n",
    "    features = {\n",
    "        \"wavelet_energy\": wavelet_energy\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def wavelet_denoise(data, wavelet, level):\n",
    "    # 小波变换\n",
    "    coeff = pywt.wavedec(data, wavelet, mode='per', level=level)\n",
    "    \n",
    "    # 计算阈值\n",
    "    sigma = np.median(np.abs(coeff[-1])) / 0.6745\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "    \n",
    "    # 阈值处理\n",
    "    coeff[1:] = [pywt.threshold(i, value=threshold, mode='soft') for i in coeff[1:]]\n",
    "    \n",
    "    # 重构信号\n",
    "    reconstructed_signal = pywt.waverec(coeff, wavelet, mode='per')\n",
    "    return reconstructed_signal\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = scipy.signal.butter(order, [low, high], btype='band')\n",
    "    y = scipy.signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_bandstop_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = scipy.signal.butter(order, [low, high], btype='bandstop')\n",
    "    y = scipy.signal.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    features, labels = [], []\n",
    "    # count = 0\n",
    "    for file in glob.glob(\"../data/combined_new/*/*.json\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        label = os.path.dirname(file).split(\"/\")[3]\n",
    "        # print(file_name,label)\n",
    "        piezo_signal, mic_signal = read_file(file)\n",
    "        # piezo_signal_centered = piezo_signal - np.mean(piezo_signal)\n",
    "        # mic_signal_centered = mic_signal - np.mean(mic_signal)\n",
    "        \n",
    "        piezo_fft_result = np.fft.fft(piezo_signal)\n",
    "        mic_fft_result = np.fft.fft(mic_signal)\n",
    "        # print(wavelet_features(piezo_signal))\n",
    "        # print(fft_features(piezo_fft_result))\n",
    "\n",
    "        piezo_spectral_feature = fft_features(piezo_fft_result)\n",
    "        piezo_spectral_feature = np.array(list(piezo_spectral_feature.values()))\n",
    "        piezo_wavelet_feature = wavelet_features(piezo_signal)['wavelet_energy']\n",
    "        # piezo_feature = np.concatenate((piezo_spectral_feature, piezo_wavelet_feature,np.abs(piezo_fft_result)))\n",
    "        piezo_feature = np.abs(piezo_fft_result)\n",
    "        # print(piezo_spectrial_feature_values)\n",
    "\n",
    "        mic_spectral_feature = fft_features(mic_fft_result)\n",
    "        mic_spectral_feature = np.array(list(mic_spectral_feature.values()))\n",
    "        mic_wavelet_feature = wavelet_features(mic_signal)['wavelet_energy']\n",
    "        # mic_feature = np.concatenate((mic_spectral_feature, mic_wavelet_feature,np.abs(piezo_fft_result)))\n",
    "        mic_feature = np.abs(mic_fft_result)\n",
    "\n",
    "        combined_feature = np.concatenate((piezo_feature, mic_feature))\n",
    "\n",
    "        features.append(combined_feature)\n",
    "        labels.append(label)\n",
    "    # print(features)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in glob.glob(\"../data/combined_new/water-bottle/*.json\"):\n",
    "#     file_name = os.path.basename(file)\n",
    "#     piezo_signal, mic_signal = read_file(file)\n",
    "#     signal_visualization(piezo_signal, mic_signal, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Proceed with scaling or other ML preparations\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 12\u001b[0m features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(features_array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    874\u001b[0m     X,\n\u001b[1;32m    875\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    876\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    877\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    878\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    945\u001b[0m         )\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming load_data() loads your features and labels\n",
    "features, labels = load_data()\n",
    "\n",
    "# Now padded_features should be convertible to a numpy array\n",
    "features_array = np.array(features)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Proceed with scaling or other ML preparations\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(features_scaled, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train a model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# # Output the predictions and their corresponding true labels\n",
    "# for i, (pred, label) in enumerate(zip(predictions, y_test)):\n",
    "#     print(f\"Sample {i}: Prediction = {pred}, True Label = {label}\")\n",
    "\n",
    "print(f'Support RandomForestClassifier\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
    "print(f'Support RandomForestClassifier\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: None, Max Proba: 0.5930, All Probas: [0.205 0.593 0.202]\n",
      "Prediction: None, Max Proba: 0.5890, All Probas: [0.178 0.233 0.589]\n",
      "Prediction: None, Max Proba: 0.5790, All Probas: [0.115 0.579 0.306]\n",
      "Prediction: Tap, Max Proba: 0.6630, All Probas: [0.22  0.117 0.663]\n",
      "Prediction: Single-Finger-Tap, Max Proba: 0.7910, All Probas: [0.181 0.791 0.028]\n",
      "Prediction: None, Max Proba: 0.4950, All Probas: [0.495 0.196 0.309]\n",
      "Prediction: Tap, Max Proba: 0.6440, All Probas: [0.23  0.126 0.644]\n",
      "Prediction: None, Max Proba: 0.4840, All Probas: [0.208 0.308 0.484]\n",
      "Prediction: Tap, Max Proba: 0.6350, All Probas: [0.111 0.254 0.635]\n",
      "Prediction: Fist, Max Proba: 0.7760, All Probas: [0.776 0.126 0.098]\n",
      "Prediction: None, Max Proba: 0.5730, All Probas: [0.212 0.573 0.215]\n",
      "Prediction: None, Max Proba: 0.4110, All Probas: [0.383 0.206 0.411]\n",
      "Prediction: None, Max Proba: 0.5650, All Probas: [0.182 0.565 0.253]\n",
      "Prediction: None, Max Proba: 0.5820, All Probas: [0.361 0.582 0.057]\n",
      "Prediction: Fist, Max Proba: 0.6130, All Probas: [0.613 0.137 0.25 ]\n",
      "Prediction: Fist, Max Proba: 0.6290, All Probas: [0.629 0.108 0.263]\n",
      "Prediction: Fist, Max Proba: 0.8040, All Probas: [0.804 0.079 0.117]\n",
      "Prediction: Single-Finger-Tap, Max Proba: 0.6930, All Probas: [0.137 0.693 0.17 ]\n",
      "Prediction: None, Max Proba: 0.5540, All Probas: [0.554 0.228 0.218]\n",
      "Prediction: Fist, Max Proba: 0.6970, All Probas: [0.697 0.136 0.167]\n",
      "Prediction: None, Max Proba: 0.4020, All Probas: [0.266 0.332 0.402]\n",
      "Prediction: None, Max Proba: 0.4460, All Probas: [0.194 0.446 0.36 ]\n",
      "Prediction: Single-Finger-Tap, Max Proba: 0.7520, All Probas: [0.203 0.752 0.045]\n",
      "Prediction: Tap, Max Proba: 0.7470, All Probas: [0.157 0.096 0.747]\n",
      "Prediction: Tap, Max Proba: 0.6010, All Probas: [0.182 0.217 0.601]\n",
      "Prediction: Single-Finger-Tap, Max Proba: 0.6800, All Probas: [0.176 0.68  0.144]\n",
      "Prediction: Single-Finger-Tap, Max Proba: 0.7470, All Probas: [0.176 0.747 0.077]\n",
      "Prediction: Fist, Max Proba: 0.7490, All Probas: [0.749 0.196 0.055]\n",
      "Prediction: None, Max Proba: 0.5080, All Probas: [0.323 0.169 0.508]\n",
      "Prediction: Fist, Max Proba: 0.7040, All Probas: [0.704 0.129 0.167]\n",
      "Accuracy: 1.0\n",
      "['None', 'None', 'None', 'Tap', 'Single-Finger-Tap', 'None', 'Tap', 'None', 'Tap', 'Fist', 'None', 'None', 'None', 'None', 'Fist', 'Fist', 'Fist', 'Single-Finger-Tap', 'None', 'Fist', 'None', 'None', 'Single-Finger-Tap', 'Tap', 'Tap', 'Single-Finger-Tap', 'Single-Finger-Tap', 'Fist', 'None', 'Fist']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features=[]\n",
    "labels=[]\n",
    "for file in glob.glob(\"data/5-23-gestures/*.json\"):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for d in data:\n",
    "            features.append(d)\n",
    "            labels.append(os.path.basename(file).split('.')[0])\n",
    "\n",
    "# Now padded_features should be convertible to a numpy array\n",
    "features_array = np.array(features)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Proceed with scaling or other ML preparations\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_array)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 预测并打印概率\n",
    "def predict_with_threshold(model, X, threshold=0.6):\n",
    "    probas = model.predict_proba(X)\n",
    "    max_probas = np.max(probas, axis=1)\n",
    "    predictions = model.predict(X)\n",
    "    results = []\n",
    "    for pred, max_proba, proba in zip(predictions, max_probas, probas):\n",
    "        if max_proba >= threshold:\n",
    "            result = (pred, max_proba, proba)\n",
    "        else:\n",
    "            result = (\"None\", max_proba, proba)\n",
    "        results.append(result)\n",
    "        print(f\"Prediction: {result[0]}, Max Proba: {result[1]:.4f}, All Probas: {result[2]}\")\n",
    "    return [result[0] for result in results]\n",
    "\n",
    "# 设置置信度阈值\n",
    "threshold = 0.6\n",
    "# 对测试集进行预测\n",
    "y_pred = predict_with_threshold(model, X_test, threshold=threshold)\n",
    "\n",
    "\n",
    "# 计算准确度（排除返回 \"None\" 的情况）\n",
    "filtered_y_test = [y for y, pred in zip(y_test, y_pred) if pred != \"None\"]\n",
    "filtered_y_pred = [pred for pred in y_pred if pred != \"None\"]\n",
    "accuracy = accuracy_score(filtered_y_test, filtered_y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# 输出预测结果\n",
    "print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
